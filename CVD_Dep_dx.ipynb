{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "id": "c0s8_v5HUTGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"\"  # Replace with your GCS bucket name\n",
        "file_path = \"\"  # Replace with your file's path in the bucket\n",
        "\n",
        "# Download file from GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "blob.download_to_filename(\"\")  # Save locally\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "-J_RIqI4cfOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Screening simulation with threshold =1"
      ],
      "metadata": {
        "id": "5A7mnxfpFOvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama 3.1 8B"
      ],
      "metadata": {
        "id": "qUywgDZLUjTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Define a function to call the Llama model\n",
        "def llama_model_fn(text: str) -> Tuple[int, str]:\n",
        "    try:\n",
        "        system_instruction = \"Message is a conversation between a patient and a healthcare professional\"\n",
        "        full_prompt = (\n",
        "            f\"{system_instruction}\\n\"\n",
        "            f\"Classify the following message as 1 if the patient shows depressive symptoms at least once,\"\n",
        "              \"otherwise 0, and provide the reasoning.\\n\"\n",
        "            f\"Output format:\\n\"\n",
        "            f\"Classification: <0 or 1>\\n\"\n",
        "            f\"Reason: <reason_text>\\n\\n\"\n",
        "            f\"Message:\\n{text}\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        )\n",
        "\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(f\"Extracted content: {content}\")  # Debug: Print extracted content\n",
        "\n",
        "        # Initialize default values\n",
        "        classification = 0\n",
        "        reasoning = \"No reason provided.\"\n",
        "\n",
        "        # Parse the response\n",
        "        parts = content.split('\\n')\n",
        "        for part in parts:\n",
        "            if \"Classification:\" in part:\n",
        "                try:\n",
        "                    classification = int(part.split(\":\", 1)[1].strip())\n",
        "                except ValueError:\n",
        "                    classification = 0  # Default to 0 in case of error\n",
        "            elif \"Reason:\" in part:\n",
        "                reasoning = part.split(\":\", 1)[1].strip()\n",
        "\n",
        "        print(f\"Parsed values -> Classification: {classification}, Reason: {reasoning}\")\n",
        "        return classification, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing message: {e}\")\n",
        "        return 0, \"Error processing message.\"\n",
        "\n",
        "# Initialize lists to store classifications and reasons\n",
        "classifications = []\n",
        "reasons = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for index, row in data.iterrows():\n",
        "    message = row.get(\"combined_msg_txt\", \"\")  # Safely get 'msg' column\n",
        "    if not message:\n",
        "        print(f\"Row {index} has no message. Skipping.\")\n",
        "        classifications.append(0)\n",
        "        reasons.append(\"No message provided.\")\n",
        "        continue\n",
        "\n",
        "    classification, reasoning = llama_model_fn(message)\n",
        "    classifications.append(classification)\n",
        "    reasons.append(reasoning)\n",
        "    time.sleep(1)  # Sleep for 1 second to avoid too many requests\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if (index + 1) % 10 == 0 or (index + 1) == len(data):\n",
        "        print(f\"Processed {index + 1}/{len(data)} messages.\")\n",
        "\n",
        "# Add classifications and reasons to the dataframe\n",
        "data[\"predictions\"] = classifications\n",
        "data[\"reasoning\"] = reasons\n",
        "\n",
        "# Handle cases where classification was None or NaN\n",
        "data[\"predictions\"].fillna(0, inplace=True)\n",
        "\n",
        "# Save the results to a CSV for review\n",
        "output_file = \"\"\n",
        "data.to_csv(output_file, index=False)\n",
        "print(f\"Classifications saved to '{output_file}'.\")\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "3vK7ddZAcfLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data=pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "V50PKRD8B2gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = \"\"\n",
        "destination_blob_name = \"\"  # Change the path if needed\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "oR7KBdZ1B2dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical time to diagnose from CVD dx to DEP dx"
      ],
      "metadata": {
        "id": "lvg6QwM5YPTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df_1 = pd.read_csv(\"\")\n",
        "\n",
        "# Convert 'created_time_jittered' and 'start_date_jittered' to datetime\n",
        "df_1['cvd_start_date_jittered'] = pd.to_datetime(df_1['cvd_start_date_jittered'], format='%Y-%m-%d %H:%M:%S')\n",
        "df_1['dep_start_date_jittered'] = pd.to_datetime(df_1['dep_start_date_jittered'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "# Calculate the time difference\n",
        "df_1['difference'] = df_1['dep_start_date_jittered'] - df_1['cvd_start_date_jittered']\n",
        "\n",
        "# Convert 'difference' column to Timedelta\n",
        "df_1['difference'] = pd.to_timedelta(df_1['difference'])\n",
        "\n",
        "# Convert 'difference' column to total hours\n",
        "df_1['difference_hours'] = df_1['difference'].dt.total_seconds() / 3600\n",
        "\n",
        "# Calculate the average of the 'difference_hours' column\n",
        "average_difference_hours = df_1['difference_hours'].mean()\n",
        "\n",
        "# Convert the average back to Timedelta in hours\n",
        "average_difference = pd.to_timedelta(average_difference_hours, unit='h')\n",
        "\n",
        "# Add the average as a new column named 'average'\n",
        "df_1['average'] = average_difference\n",
        "\n",
        "data.to_csv(\"\", index=False)\n",
        "# Display the updated DataFrame\n",
        "print(df_1)\n",
        "\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_1)\n"
      ],
      "metadata": {
        "id": "JTJbVrYv3sAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### average number of messages per patient"
      ],
      "metadata": {
        "id": "gahprriNGuHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df_1 = pd.read_csv(\"\")\n",
        "\n",
        "\n",
        "# Calculate the average of the 'message_id_count' column\n",
        "msg_freq_ave = df_1['message_id_count'].mean()\n",
        "\n",
        "\n",
        "# Display the DataFrame\n",
        "print(msg_freq_ave)\n"
      ],
      "metadata": {
        "id": "OzcS8-3CI_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### accuracy computation (th=1)"
      ],
      "metadata": {
        "id": "dXdh7VheSAcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE"
      ],
      "metadata": {
        "id": "3B_YQbRN_E7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the GCS client\n",
        "client = storage.Client()\n",
        "\n",
        "# List of file paths in GCS and local paths\n",
        "bucket_name = \"\"\n",
        "gcs_file_paths = [f'path/_{i}.csv' for i in range(1, 6)]\n",
        "local_file_paths = [f\"file{i}.csv\" for i in range(1, 6)]\n",
        "\n",
        "# Download files from GCS\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "for gcs_path, local_path in zip(gcs_file_paths, local_file_paths):\n",
        "    bucket.blob(gcs_path).download_to_filename(local_path)\n",
        "\n",
        "# Read and concatenate all datasets\n",
        "dataframes = [pd.read_csv(file) for file in local_file_paths]\n",
        "case_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Add a new column 'label' and set all values to 1\n",
        "case_df['label'] = 1\n",
        "\n",
        "# Save the combined dataframe to a new file\n",
        "case_df.to_csv(\"\", index=False)\n",
        "\n",
        "print(\"Datasets combined and label column added successfully!\")"
      ],
      "metadata": {
        "id": "QdWMuz1U_E4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTROL"
      ],
      "metadata": {
        "id": "8lMX7eqI_Esm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the GCS client\n",
        "client = storage.Client()\n",
        "\n",
        "# List of file paths in GCS and local paths\n",
        "bucket_name = \"\"\n",
        "gcs_file_paths = [f'path/_{i}.csv' for i in range(1, 6)]\n",
        "local_file_paths = [f\"file{i}.csv\" for i in range(1, 6)]\n",
        "\n",
        "# Download files from GCS\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "for gcs_path, local_path in zip(gcs_file_paths, local_file_paths):\n",
        "    bucket.blob(gcs_path).download_to_filename(local_path)\n",
        "\n",
        "# Read and concatenate all datasets\n",
        "dataframes = [pd.read_csv(file) for file in local_file_paths]\n",
        "control_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Add a new column 'label' and set all values to 0\n",
        "control_df['label'] = 0\n",
        "\n",
        "# Save the combined dataframe to a new file\n",
        "control_df.to_csv(\"\", index=False)\n",
        "\n",
        "print(\"Datasets combined and label column added successfully!\")"
      ],
      "metadata": {
        "id": "Wtbr7xIi__v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined case and control (when, th=1)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# File paths for the two datasets\n",
        "file1 = \"\"\n",
        "file2 = \"\"\n",
        "\n",
        "# Read the datasets into pandas DataFrames\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "# Concatenate the two DataFrames\n",
        "comb_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "\n",
        "# Save the combined DataFrame to a new file\n",
        "comb_df.to_csv(\"\", index=False)\n",
        "\n",
        "print(\"Files concatenated and label column added successfully!\")"
      ],
      "metadata": {
        "id": "A3RqsgBbEZgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the file in the bucket\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = \"\"\n",
        "destination_blob_name = \"\"  # Change the path if needed\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "cOhRar_t45M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification test"
      ],
      "metadata": {
        "id": "SBg3_xZ6oCxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH 95% CI"
      ],
      "metadata": {
        "id": "PQjsdP1voCxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv(\"\")\n",
        "\n",
        "# Extract the 'label' and 'classification' columns\n",
        "y_true = df['label']\n",
        "y_pred = df['predictions']\n",
        "\n",
        "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_bootstraps=1000, ci=95, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval for a given metric using bootstrapping.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (pd.Series): True labels.\n",
        "        y_pred (pd.Series): Predicted labels.\n",
        "        metric_func (function): Scikit-learn metric function to calculate (e.g., f1_score).\n",
        "        n_bootstraps (int): Number of bootstrap samples.\n",
        "        ci (float): Confidence level (e.g., 95 for 95% CI).\n",
        "        **kwargs: Additional keyword arguments for the metric function.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    boot_scores = []\n",
        "    n = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        # Sample with replacement\n",
        "        indices = np.random.randint(0, n, n)\n",
        "        y_true_boot = y_true.iloc[indices]\n",
        "        y_pred_boot = y_pred.iloc[indices]\n",
        "\n",
        "        # Handle cases where metric might fail (e.g., no positive predictions)\n",
        "        try:\n",
        "            score = metric_func(y_true_boot, y_pred_boot, **kwargs)\n",
        "            boot_scores.append(score)\n",
        "        except ValueError:\n",
        "            continue  # Skip this bootstrap sample if metric calculation fails\n",
        "\n",
        "    # Calculate percentiles for the confidence interval\n",
        "    lower = np.percentile(boot_scores, (100 - ci) / 2)\n",
        "    upper = np.percentile(boot_scores, 100 - (100 - ci) / 2)\n",
        "    return lower, upper\n",
        "\n",
        "# Calculate the point estimates for the metrics\n",
        "f1 = f1_score(y_true, y_pred, average='binary')  # Adjust 'average' as needed\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the 95% confidence intervals using bootstrapping\n",
        "f1_ci = bootstrap_confidence_interval(y_true, y_pred, f1_score, average='binary')\n",
        "precision_ci = bootstrap_confidence_interval(y_true, y_pred, precision_score, average='binary')\n",
        "recall_ci = bootstrap_confidence_interval(y_true, y_pred, recall_score, average='binary')\n",
        "accuracy_ci = bootstrap_confidence_interval(y_true, y_pred, accuracy_score)\n",
        "\n",
        "# Print the results\n",
        "print(f'F1 Score: {f1:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})')\n",
        "print(f'Precision: {precision:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})')\n",
        "print(f'Recall: {recall:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})')\n",
        "print(f'Accuracy: {accuracy:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})')"
      ],
      "metadata": {
        "id": "dfQt3a-soCxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changes in time to diagnosis (threshold=3)"
      ],
      "metadata": {
        "id": "QOKAvsxvoZd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when threshold = 3"
      ],
      "metadata": {
        "id": "gwiw12B8O9kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = pd.read_csv(\"\")\n",
        "\n",
        "# Create a DataFrame\n",
        "df_CVD_Dep= pd.DataFrame(data)\n",
        "\n",
        "# Convert 'created_time_jittered' and 'start_date_jittered' to datetime\n",
        "df_CVD_Dep['created_time_jittered'] = pd.to_datetime(df_CVD_Dep['created_time_jittered'], format='%Y-%m-%d %H:%M:%S')\n",
        "df_CVD_Dep['dep_start_date_jittered'] = pd.to_datetime(df_CVD_Dep['dep_start_date_jittered'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Identify the second occurrence of prediction = 1 for each ID\n",
        "df_CVD_Dep['occurrence'] = df_CVD_Dep[df_CVD_Dep['predictions'] == 1].groupby('anon_id').cumcount() + 1\n",
        "\n",
        "# Calculate the time difference\n",
        "df_CVD_Dep['difference'] = None  # Initialize the 'difference' column with None\n",
        "df_CVD_Dep.loc[(df_CVD_Dep['predictions'] == 1) & (df_CVD_Dep['occurrence'] == 3), 'difference'] = df_CVD_Dep['dep_start_date_jittered'] - df_CVD_Dep['created_time_jittered']\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_CVD_Dep.to_csv(\"\", index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_CVD_Dep)\n",
        "\n"
      ],
      "metadata": {
        "id": "iqDEyX4JoZd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows with NaN values in the 'difference' column\n",
        "real_difference_CVD_Dep = df_CVD_Dep.dropna(subset=['difference'])\n",
        "\n",
        "# Convert 'difference' column to Timedelta\n",
        "real_difference_CVD_Dep['difference'] = pd.to_timedelta(real_difference_CVD_Dep['difference'])\n",
        "\n",
        "# Convert 'difference' column to total hours\n",
        "real_difference_CVD_Dep['difference_hours'] = real_difference_CVD_Dep['difference'].dt.total_seconds() / 3600\n",
        "\n",
        "# Calculate the average of the 'difference_hours' column\n",
        "average_difference_hours = real_difference_CVD_Dep['difference_hours'].mean()\n",
        "\n",
        "# Convert the average back to Timedelta in hours\n",
        "average_difference = pd.to_timedelta(average_difference_hours, unit='h')\n",
        "\n",
        "# Add the average as a new column named 'average'\n",
        "real_difference_CVD_Dep['average'] = average_difference\n",
        "\n",
        "data.to_csv(\"\", index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(real_difference_CVD_Dep)\n"
      ],
      "metadata": {
        "id": "wezvMnZ2oZd6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}